{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "602fec09-a231-4855-bab7-19410340e532",
   "metadata": {},
   "source": [
    "# Exercise 4\n",
    "Due:  Tue November 19, 8:00am"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e31e5",
   "metadata": {},
   "source": [
    "## Node2Vec\n",
    "1. Implement custom dataset that samples pq-walks\n",
    "    - Use the utility function from torch_cluster that actually performs the walks\n",
    "2. Implement Node2Vec module and training\n",
    "\t- Node2Vec essentially consists of a torch.Embedding module and a loss function\n",
    "3. Evaluate node classification performance on Cora\n",
    "4. Evaluate on Link Prediction: Cora, PPI\n",
    "    - use different ways to combine the node two embeddings for link prediction\n",
    "\n",
    "Bonus Question: are the predictions stable wrt to the random seeds of the walks?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497bd917-87df-4c9c-8d69-7dd0cd90212a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cac6c9e9-fc1f-4a34-8404-165315a2ebf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric as pyg\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import IterableDataset, DataLoader, Dataset\n",
    "from torch_cluster import random_walk\n",
    "import sklearn\n",
    "import torch_scatter \n",
    "import torch_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22cff34d-205e-4ebe-a747-a36fa653895a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find device\n",
    "if torch.cuda.is_available(): # NVIDIA\n",
    "    device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available(): # apple M1/M2\n",
    "    device = torch.device('mps') \n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c275ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Akshat\\anaconda3\\envs\\lua_torch\\lib\\site-packages\\torch_geometric\\data\\dataset.py:238: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_transform):\n",
      "c:\\Users\\Akshat\\anaconda3\\envs\\lua_torch\\lib\\site-packages\\torch_geometric\\data\\dataset.py:246: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  if osp.exists(f) and torch.load(f) != _repr(self.pre_filter):\n",
      "c:\\Users\\Akshat\\anaconda3\\envs\\lua_torch\\lib\\site-packages\\torch_geometric\\io\\fs.py:215: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(f, map_location)\n"
     ]
    }
   ],
   "source": [
    "dataset = pyg.datasets.Planetoid(root='./dataset/cora', name='Cora')\n",
    "cora = dataset[0]\n",
    "dataset = pyg.datasets.PPI(root='./dataset/ppi')\n",
    "ppi = dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378d4a84-bda3-484a-87ca-35f95344c594",
   "metadata": {},
   "source": [
    "## node2vec embedding training\n",
    "Here the main training and everything on the graph level is happening.\n",
    "\n",
    "It might be a good idea to create a dataset of walks (fixed for the whole training process) first to get the whole training process running before attempting to create a train_loader that on-demand samples those walks on-demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb739f66-a29d-435a-a2e3-66287f1e4422",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node2VecDataset(Dataset):\n",
    "    def __init__(self, data, p=1.0, q=1.0, walk_length=10, num_walks=10):\n",
    "        self.data = data\n",
    "        self.edge_index = data.edge_index\n",
    "        self.num_nodes = data.num_nodes\n",
    "        self.p = p\n",
    "        self.q = q\n",
    "        self.walk_length = walk_length\n",
    "        self.num_walks = num_walks\n",
    "\n",
    "        row, col = self.edge_index\n",
    "        \n",
    "        # Precompute the walks\n",
    "        self.walks = []\n",
    "        for _ in range(self.num_walks):\n",
    "            start_nodes = torch.arange(self.num_nodes)\n",
    "            walks = random_walk(row, col, start_nodes, walk_length=self.walk_length)\n",
    "            self.walks.append(walks)\n",
    "        self.walks = torch.cat(self.walks, dim=0)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.walks)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.walks[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "140000be",
   "metadata": {},
   "outputs": [],
   "source": [
    "cora_num_classes = cora.y.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "81fd688c-009d-4af5-9c16-f3878701235e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(Node2VecDataset(cora), shuffle=True, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5f114591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Node2Vec(nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim):\n",
    "        super(Node2Vec, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_nodes = num_nodes\n",
    "\n",
    "        # Embedding for nodes (input)\n",
    "        self.embedding = nn.Embedding(num_nodes, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.embedding.weight)\n",
    "\n",
    "        # Embedding for nodes (output)\n",
    "        self.context_embedding = nn.Embedding(num_nodes, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.context_embedding.weight)\n",
    "\n",
    "    def forward(self, nodes):\n",
    "        node_embeds = self.embedding(nodes)\n",
    "        return node_embeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e122c722",
   "metadata": {},
   "outputs": [],
   "source": [
    "def negative_sampling_loss(center_embeddings, context_embeddings, negative_embeddings):\n",
    "    # Positive score: dot product between center and context embeddings\n",
    "    pos_score = torch.sum(center_embeddings * context_embeddings, dim=-1)\n",
    "    pos_loss = F.logsigmoid(pos_score).squeeze()\n",
    "\n",
    "    # Negative score: dot product between center and negative embeddings\n",
    "    neg_score = torch.bmm(negative_embeddings, center_embeddings.unsqueeze(2)).squeeze()\n",
    "    neg_loss = F.logsigmoid(-neg_score).sum(1)\n",
    "\n",
    "    loss = -(pos_loss + neg_loss).mean()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a3d981b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def train_node2vec(model, data_loader, num_nodes, epochs=10, window_size=5, negative_samples=5, lr=0.01, device='cpu'):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    model.to(device)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        for walks in data_loader:\n",
    "            walks = walks.to(device)\n",
    "            batch_size, walk_length = walks.shape\n",
    "\n",
    "            # For each position in the walk\n",
    "            for pos in range(walk_length):\n",
    "                center_nodes = walks[:, pos]\n",
    "\n",
    "                # Determine context window\n",
    "                start = max(0, pos - window_size)\n",
    "                end = min(walk_length, pos + window_size + 1)\n",
    "                context_positions = list(range(start, pos)) + list(range(pos + 1, end))\n",
    "                context_nodes = walks[:, context_positions]\n",
    "\n",
    "                # Flatten context nodes\n",
    "                context_nodes = context_nodes.reshape(-1)\n",
    "\n",
    "                # Get embeddings\n",
    "                center_embeddings = model.embedding(center_nodes)\n",
    "                context_embeddings = model.context_embedding(context_nodes)\n",
    "\n",
    "                # Negative sampling\n",
    "                negative_nodes = torch.randint(0, num_nodes, (batch_size * len(context_positions), negative_samples), device=device)\n",
    "                negative_embeddings = model.context_embedding(negative_nodes)\n",
    "\n",
    "                # Compute loss\n",
    "                loss = negative_sampling_loss(center_embeddings.repeat_interleave(len(context_positions), dim=0),\n",
    "                                              context_embeddings,\n",
    "                                              negative_embeddings)\n",
    "\n",
    "                # Backpropagation\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(data_loader)\n",
    "        print(f'Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e32ea4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "embedding_dim = 128\n",
    "window_size = 5\n",
    "negative_samples = 5\n",
    "epochs = 10\n",
    "lr = 0.01\n",
    "batch_size = 128\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915e34dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = cora.num_nodes\n",
    "model = Node2Vec(num_nodes, embedding_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "52d34215",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Akshat\\AppData\\Local\\Temp\\ipykernel_5624\\2919600246.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('./node2vec_model.pth'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('./node2vec_model.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32302fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 60.0469\n",
      "Epoch 2/10, Loss: 55.0850\n",
      "Epoch 3/10, Loss: 54.4130\n",
      "Epoch 4/10, Loss: 54.1472\n",
      "Epoch 5/10, Loss: 53.9153\n",
      "Epoch 6/10, Loss: 53.7944\n",
      "Epoch 7/10, Loss: 53.7160\n",
      "Epoch 8/10, Loss: 53.6526\n",
      "Epoch 9/10, Loss: 53.6061\n",
      "Epoch 10/10, Loss: 53.5482\n"
     ]
    }
   ],
   "source": [
    "node2vec_dataset = Node2VecDataset(cora, p=1.0, q=1.0, walk_length=80, num_walks=10)\n",
    "data_loader = DataLoader(node2vec_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_node2vec(model, data_loader, num_nodes, epochs=epochs, window_size=window_size,\n",
    "               negative_samples=negative_samples, lr=lr, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "5962ac1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (model)\n",
    "torch.save(model.state_dict(), 'node2vec_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bfe71e5-f122-46ff-abdd-7b0fd15d56fc",
   "metadata": {},
   "source": [
    "## Node classification performance\n",
    "just a small MLP or even linear layer on the embeddings to predict node classes. Accuracy should be above 60%. Please compare your results to those you achieved with GNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3ca95d6c-ed3f-49eb-b733-17a9035db399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# as the simple MLP is pretty straightforward\n",
    "cls_model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(embedding_dim, 256), # Input layer\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(256, 128), # Hidden layer 2\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(128, cora_num_classes), # Output layer\n",
    ")\n",
    "\n",
    "cls_model = cls_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "68b48d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f648c85-bf32-42f0-94f0-19e052f70325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.724e+00\n",
      "Epoch 20, Loss: 1.224e+00\n",
      "Epoch 30, Loss: 5.340e-01\n",
      "Epoch 40, Loss: 1.311e-01\n",
      "Epoch 50, Loss: 2.840e-02\n",
      "Epoch 60, Loss: 9.375e-03\n",
      "Epoch 70, Loss: 4.949e-03\n",
      "Epoch 80, Loss: 3.428e-03\n",
      "Epoch 90, Loss: 2.718e-03\n",
      "Epoch 100, Loss: 2.299e-03\n",
      "Epoch 110, Loss: 2.007e-03\n",
      "Epoch 120, Loss: 1.782e-03\n",
      "Epoch 130, Loss: 1.598e-03\n",
      "Epoch 140, Loss: 1.443e-03\n",
      "Epoch 150, Loss: 1.310e-03\n",
      "Epoch 160, Loss: 1.194e-03\n",
      "Epoch 170, Loss: 1.094e-03\n",
      "Epoch 180, Loss: 1.005e-03\n",
      "Epoch 190, Loss: 9.264e-04\n",
      "Epoch 200, Loss: 8.567e-04\n",
      "node classification accuracy for cora: 0.58 (train: 1.00, val: 0.58)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(cls_model.parameters(), lr=0.001)  # define an optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss()  # define loss function\n",
    "\n",
    "node2vec_embeddings = model.embedding.weight.to(device)\n",
    "cora = cora.to(device)\n",
    "\n",
    "for epoch in range(200):  # 100 epochs\n",
    "    cls_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = cls_model(node2vec_embeddings[cora.train_mask])  # forward pass\n",
    "    loss = criterion(out, cora.y[cora.train_mask]) \n",
    "    loss.backward()  \n",
    "    optimizer.step()\n",
    "\n",
    "    # print out loss info\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.3e}\")\n",
    "\n",
    "def get_accuracy(cls_model, embeddings, y, mask):\n",
    "    out = cls_model(embeddings[mask])\n",
    "    pred = out.argmax(dim=1)\n",
    "    acc = accuracy_score(y[mask].cpu().numpy(), pred.cpu().detach().numpy())\n",
    "    return acc\n",
    "\n",
    "train_acc = get_accuracy(cls_model, node2vec_embeddings, cora.y, cora.train_mask)\n",
    "val_acc = get_accuracy(cls_model, node2vec_embeddings, cora.y, cora.val_mask)\n",
    "test_acc = get_accuracy(cls_model, node2vec_embeddings, cora.y, cora.test_mask)\n",
    "    \n",
    "print(f\"node classification accuracy for cora: {test_acc:.2f} (train: {train_acc:.2f}, val: {val_acc:.2f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7712ab1-4753-4715-abaf-33b666680535",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378a139-b47e-49a1-a65b-90e7a96ca165",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "83edad57-6fa5-466a-b3d7-56244a79f410",
   "metadata": {},
   "source": [
    "## link prediction on trained embeddings\n",
    "this should only train simple MLPs.\n",
    "\n",
    "Note: for link prediction to be worthwhile, one needs to train the embeddings on a subset of the graph (less edges, same nodes) instead of the whole graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "290ffae9-7872-441a-9e63-385619295400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 7392], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[7392], edge_label_index=[2, 7392])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for link prediction, do something like the following\n",
    "link_splitter = pyg.transforms.RandomLinkSplit(is_undirected=True)\n",
    "train_data, val_data, test_data = link_splitter(cora)\n",
    "train_data\n",
    "# the positive and negative edges are in \"edge_label_index\" with \"edge_label\" \n",
    "# indicating whether an edge is a true edge or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4e295578-2a40-4eba-a580-8cc7502492f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[2708, 1433], edge_index=[2, 8446], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708], edge_label=[2110], edge_label_index=[2, 2110])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc451bdf-859d-4592-9708-43c93aa8fbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrain node2vec on train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7cdc42-69e8-4320-b5df-c044ea48b52f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9f11bab-e219-4434-9e2d-16b2e3c702c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use those (new) embeddings for link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a632698-60fd-40d6-b3c3-d924402f24e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c840653f-8348-4792-86fd-59679ee523bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f0649e-7dbc-441b-b019-1f7239a5c5c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lua_torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
